% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bacchuss.R
\name{bacchuss}
\alias{bacchuss}
\title{Annotate texts according to coding instructions}
\usage{
bacchuss(
  df,
  input_column = "text",
  ctx_column = "estimated_context_length",
  instructions,
  examples = NULL,
  explanations = NULL,
  codes = NULL,
  reminder_s = "",
  reminder,
  expected_response_format = c("plain", "json"),
  model,
  host = NULL,
  api_key = NULL,
  backend = c("ollama", "litellm"),
  temperature = 0.2,
  seed = NULL,
  retry_loop = TRUE,
  retry_sleep = 30,
  max_retries = 3,
  default_ctx = 5120,
  warmup = 0
)
}
\arguments{
\item{df}{A data frame containing the texts you want to annotate}

\item{input_column}{A char indicating the column the texts to code
are stored. Default: "text"}

\item{ctx_column}{A char indicating where the context length for
the call is stored. Leave at default if there is none.
Default: "estimated_context_length". If that column doesn't exist:
Throws a message that it uses default_ctx, then uses default_ctx.
Can go up to 40960 if your llm allows it.}

\item{instructions}{A char with your coding instructions}

\item{examples, codes}{Two vectors with annotation examples: the
example text and the correct label for each case}

\item{explanations}{Leave Null if you don't want explanations for
each label.
   * If you use zero shot and want explanations, set TRUE
   * If you use few shot, give a vector with explanations for the
   example labels}

\item{reminder_s}{Short reminders to send after each coding example.
Can be left empty, does not usually improve reliability and costs
too many tokens}

\item{reminder}{Short reminder sent after all coding examples. Can
Increase stability of labels - repeat instructions for available labels,
rules the model needs reminder of (for example "do not interpret, only
go by what's explicit in the text").}

\item{expected_response_format}{Do you use plaintext or json format? Default plain.}

\item{model}{Which LLM model to use. No default.}

\item{host}{Address of the host you use. Default: local ollama
installation (uses default from ollamar package)}

\item{api_key}{Which API key to use (if your host handes users with
API keys). Default: NULL for unrestricted APIs.}

\item{backend}{Are you using a local Ollama instance or a LiteLLM host?}

\item{temperature}{Which temperature to use for the LLM. Higher =
more creative, lower = more consistent. Anything above 1 is not advised.
Default set to 0.2.}

\item{seed}{Seed to use for deterministic annotation. In unsuccessful
tries, will try again with seed + 1. Default: NULL}

\item{retry_loop}{Small LLMs sometimes get into a long loop producing irrelevant
text. If set TRUE, the results of this run will be discarded and the LLM will
rerun with seed + 1. If set FALSE, keep result from loop - you can inspect the
raw output to check what happened.}

\item{retry_sleep}{How long to wait before retrying - to avoid over-
loading local ollama instances. Default: 30 seconds.}

\item{max_retries}{Max number of retries per call. Usually only needs
one retry. Default: 3.}

\item{default_ctx}{Default context length if no context length is handed
over. Default: 5120.}

\item{warmup}{Ollama sometimes acts non-deterministically even if you set a seed.
to adjust to that you can hand over a warmup-value - the first cases will be run
in a warm-up loop before the system runs the actual cases. Default: 0}
}
\value{
A data frame with added columns:
\describe{
  \item{labels}{Annotated labels.}
  \item{explanations}{Explanations when requested.}
  \item{raw_output}{Full LLM output per row.}
  \item{max_context_used}{Context used.}
  \item{tokens_prompt}{Prompt tokens.}
  \item{tokens_response}{Response tokens.}
}
}
\description{
`bacchuss()` annotates the text in the dataframe you hand over.
It uses the instructions you hand over, and detects whether you added
example cases and explanations. Please write instructions that tell the
llm to output labels after Labels:
If your instructions tell the llm to output an explanation first, and
set explanation = TRUE, or hand over a set of explanations for your few shot
examples. bacchuss returns everything after Explanation: as well.
Sometimes, small LLMs will be stuck in short loops. Default behavior is to
message "Attempt failed" and then retry.
}
