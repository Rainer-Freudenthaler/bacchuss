% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bacchuss.R
\name{bacchuss_satyr}
\alias{bacchuss_satyr}
\title{Annotate a single text according to coding instructions}
\usage{
bacchuss_satyr(
  instructions,
  examples = NULL,
  explanations = NULL,
  codes = NULL,
  reminder_s,
  reminder,
  input_text,
  est_ctx_len = NA,
  expected_response_format = c("plain", "json"),
  model,
  host = NULL,
  api_key = NULL,
  backend = c("ollama", "litellm"),
  temperature = 0.2,
  seed = NULL,
  retry_sleep = 30,
  max_retries = 3,
  default_ctx = 5120,
  retry_loop = TRUE
)
}
\arguments{
\item{instructions}{A char with your coding instructions}

\item{examples, codes}{Two vectors with annotation examples: the
example text and the correct label for each case}

\item{explanations}{Leave Null if you don't want explanations for
each label.
   * If you use zero shot and want explanations, set TRUE
   * If you use few shot, give a vector with explanations for the
   example labels}

\item{reminder_s}{Short reminders to send after each coding example.
Can be left empty, does not usually improve reliability and costs
too many tokens}

\item{reminder}{Short reminder sent after all coding examples. Can
Increase stability of labels - repeat instructions for available labels,
rules the model needs reminder of (for example "do not interpret, only
go by what's explicit in the text").}

\item{input_text}{The text to annotate}

\item{est_ctx_len}{The estimated context length for instructions, examples,
codes and the text to code.}

\item{expected_response_format}{Do you use plaintext or json format? Default plain.}

\item{model}{Which LLM model to use. No default.}

\item{host}{Address of the host you use. Default: local ollama
installation (uses default from ollamar package)}

\item{api_key}{Which API key to use (if your host handes users with
API keys). Default: NULL for unrestricted APIs.}

\item{backend}{Are you using a local Ollama instance or a LiteLLM host?}

\item{temperature}{Which temperature to use for the LLM. Higher =
more creative, lower = more consistent. Anything above 1 is not advised.
Default set to 0.2.}

\item{seed}{Seed to use for deterministic annotation. In unsuccessful
tries, will try again with seed + 1. Default: NULL}

\item{retry_sleep}{How long to wait before retrying - to avoid over-
loading local ollama instances. Default: 30 seconds.}

\item{max_retries}{Max number of retries per call. Usually only needs
one retry. Default: 3.}

\item{default_ctx}{Default context length if no context length is handed
over. Default: 5120.}

\item{retry_loop}{Small LLMs sometimes get into a long loop producing irrelevant
text. If set TRUE, the results of this run will be discarded and the LLM will
rerun with seed + 1. If set FALSE, keep result from loop - you can inspect the
raw output to check what happened.}
}
\value{
A list with:
\describe{
  \item{label}{Annotated label for the text.}
  \item{explanation}{Explanation when requested.}
  \item{raw_output}{Full LLM output.}
  \item{max_context_used}{Context sent to the model.}
  \item{tokens_prompt}{Prompt token count.}
  \item{tokens_response}{Response token count.}
}
}
\description{
`bacchuss_satyr()` annotates a single text. It's used within `bacchuss()`.
It uses the instructions you hand over, and detects whether you added
example cases and explanations. Please write instructions that tell the
llm to output labels after Labels:
If your instructions tell the llm to output an explanation first, and
set explanation = TRUE, or hand over a set of explanations for your few shot
examples. bacchuss_satyr returns everything after Explanation: as well.
}
